# 프토토타입(초기 버전)

6~8점 구간을 집중해서 보면, 빈 공간으로 갈 때 죽지 않는다는 확실한 수가 있다보니 바로 옆에 먹이가 있음에도 먹이 주변을 빙빙 돌기만 하는 장면을 어렵지 않게 볼 수 있다.

TOP 점수 20점을 넘기기 시작한 이후로는 점점 진전이 느려지고 있다. 개인적인 생각으론 탐험의 확률이 높다보니 17-18점 넘길 고비까지는 확실한 수가 있어도 탐험을 하다가 죽는 경우가 한 번 정도는 찾아올 가능성이 높아지면서 20점을 넘기기는 힘들어하는 거 같다.

Epoch 1400~1700 구간에 보면 최근 100 epoch 점수가 7점대까지 떨어지는 걸 볼 수 있다. 이건 왜 그런 걸까?

현재 1700대 에포크를 보고 있는데 아직도 0점으로 종료하는 게임이 10번에 한 번은 꼭 나오고 있다. 여전히 벽 또는 몸과의 상관 관계에 적응하지 못한 걸까? 아니면 epsilon에 의한 탐험이 악수로 작용하는 특이한 상황일까?
보통 15점 이상까지 큰 무리 없이 가는 걸 보면 아마 탐험에 의한 특이한 상황일 거라고 추측해본다.

2천대 후반 에포크로 접어들면서 100번에 한 번 정도만 0점으로 종료하는 게임이 생기고 있다. 현재 3,200에포크 기준 최고 점수는 33점. 최근 100에포크 평균은 14점 내외이다.

3천대 중반 에포크에서 최근 100 에포크 평균 점수가 12점대까지 내려갔다가 다시 13점대까지 회복됐다. 현재 3,700대 12점까지 내려갔다가 다시 13점을 회복했는데 현재는 아직 평균 13점 내외를 웃도는 거 같다.
이전 Greedy-Algorithm에서 봤을 때 10x10 그리드 기준으로 21점이 넘으면 (0, y)->(9, y)->(9, y±1)->(0, y±1)처럼 한 번 크게 돌아서 움직여도 퇴로가 막힐 확률이 생기기 시작하는데, 그때부터는 랜덤성이 존재하는 먹이의 위치에 따라 몇 점까지 갈지가 정해지고 AI의 지능 영향은 크게 안받는 걸로 보였다. 지금 DQN AI도 비슷한 느낌인 거 같다.

4,200 에포크에 도달하니 과적합 문제가 발생한 건지 최근 100 에포크 평균 점수가 6점 초반대까지 내려오는 문제가 발생했다.

현재 5,200 에포크 째, 먹이 방향으로 찾아가는 건 이제 익숙해진 듯 하다. 하지만 자기 몸으로 만들어진 외진 길에 빠져 죽는 경우가 빈번하게 발생하고 있다. 우선 에포크를 반복하다보면 해결될지 한 번 하룻밤 냅둬보겠다.

# 개선 방안 적용
- Epsilon scheduling을 지연시키기: epsilon_decay를 0.995에서 0.999로 변경. epsilon_decay를 매 스텝마다 적용하지 않고 5천 스텝마다 적용하도록 변경.
- `ReplayBuffer.sample()` 함수가 학습 데이터를 완전히 random으로 가져오도록 하지 않고 최신 데이터일수록 좀 더 가중치가 부여되도록 개선
- reward system 개선: 먹이와의 거리에 따른 ±0.1, 먹이를 먹었을 때 +1 만 존재하던 보상 시스템에 스텝마다 생존 추가 점수 +0.01, GameOver 시 -1 을 부여하도록 변경
- state에서 목의 방향을 mapping한 int 값을 제외

24만 에포크를 실행했는데도 최고 점수 5점에 평균 점수 0.156이다. Epsilon 폭을 너무 방대하게 잡아놓은 거 같다.

# 개선 방안 적용
- Epsilon scheduling을 가속시키기: epsilon_decay를 0.999에서 다시 0.995로 변경, epsilon_decay를 5천 스텝마다 적용하지 않고 1,000 스텝마다 적용하도록 변경
- reward system 변경: 먹이와의 거리에 따른 ±0.1 -> ±0.5, 생존 추가 점수 +0.01 -> 0.1
- `ReplayBuffer.sample()` 함수가 학습 데이터를 샘플링할 때 이용하던 방식 개선: 선형 가중치 -> 로그(`log(i + 1)`) 가중치

오. 밤새 켜놓은지라 다시 영상을 봐야겠지만 55,242 에포크에 50점을 달성했다.

# MCTS(Monte Carlo Tree Search) 적용 시도

몬테카를로 트리 검색 적용 시도에 실패했다. 프로토타입 코드를 구해서 적용해봤는데 제대로 작동하지 않았다. 실력이 부족한 관계로 `Policy Gradient` 코드를 먼저 구해서 적용해보자.
사실 MCTS를 적용했어도 장기적인 문제는 결국 해결되지 않기 때문에 스킵해도 상관 없긴 하다.